{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "import random\n",
    "from pythainlp.corpus.common import thai_stopwords\n",
    "from deeple_preprocessor.tokenize import newmm_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tqdm import tqdm\n",
    "from pythainlp.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(thai_stopwords())\n",
    "#จัดการประโยคซ้ำ\n",
    "data_not=[]\n",
    "\n",
    "def remove_emojis(text):\n",
    "    pattern = re.compile(r\"[\\u0E00-\\u0E7Fa-zA-Z0-9' \\n\\-!$%^&*()_+|~=`{}\\[\\]:'<>?,.#\\/]\")\n",
    "    return ''.join(pattern.findall(text))\n",
    "def normalize_space(text):\n",
    "    text = text.replace('\\t', '')\n",
    "    text = text.replace('\\u200b', '')\n",
    "    text = text.replace('   ', ' ')\n",
    "    text = text.replace('  ', ' ')\n",
    "    return text.strip()\n",
    "\n",
    "def Unique(p):\n",
    "    text=re.sub(\"<[^>]*>\",\"\",p)\n",
    "    text=re.sub(\"\\[(.*?)\\]\",\"\",text)\n",
    "    text=re.sub(\"\\[\\/(.*?)\\]\",\"\",text)\n",
    "    if text not in data_not:\n",
    "        data_not.append(text)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "# เตรียมตัวตัด tag ด้วย re\n",
    "pattern = r'\\[(.*?)\\](.*?)\\[\\/(.*?)\\]'\n",
    "tokenizer = RegexpTokenizer(pattern) # ใช้ nltk.tokenize.RegexpTokenizer เพื่อตัด [TIME]8.00[/TIME] ให้เป็น ('TIME','ไง','TIME')\n",
    "# จัดการกับ tag ที่ไม่ได้ tag\n",
    "def toolner_to_tag(text):\n",
    "    text=text.strip().replace(\"FACILITY\",\"LOCATION\").replace(\"[AGO]\",\"\").replace(\"[/AGO]\",\"\").replace(\"[T]\",\"\").replace(\"[/T]\",\"\")\n",
    "    text=re.sub(\"<[^>]*>\",\"\",text)\n",
    "    text=re.sub(\"(\\[\\/(.*?)\\])\",\"\\\\1***\",text)#.replace('(\\[(.*?)\\])','***\\\\1')# text.replace('>','>***') # ตัดการกับพวกไม่มี tag word\n",
    "    text=re.sub(\"(\\[\\w+\\])\",\"***\\\\1\",text)\n",
    "    text2=[]\n",
    "    for i in text.split('***'):\n",
    "        if \"[\" in i:\n",
    "            text2.append(i)\n",
    "        else:\n",
    "            text2.append(\"[word]\"+i+\"[/word]\")\n",
    "    text=\"\".join(text2)#re.sub(\"[word][/word]\",\"\",\"\".join(text2))\n",
    "    return text.replace(\"[word][/word]\",\"\")\n",
    "# แปลง text ให้เป็น conll2002\n",
    "def text2conll2002(text,pos=True):\n",
    "    \"\"\"\n",
    "    ใช้แปลงข้อความให้กลายเป็น conll2002\n",
    "    \"\"\"\n",
    "    text=toolner_to_tag(text)\n",
    "    text=text.replace(\"''\",'\"')\n",
    "    text=text.replace(\"’\",'\"').replace(\"‘\",'\"')#.replace('\"',\"\")\n",
    "    tag=tokenizer.tokenize(text)\n",
    "    j=0\n",
    "    conll2002=\"\"\n",
    "    for tagopen,text,tagclose in tag:\n",
    "        word_cut=newmm_tokenize(text) # ใช้ตัวตัดคำ newmm\n",
    "        i=0\n",
    "        txt5=\"\"\n",
    "        while i<len(word_cut):\n",
    "            if word_cut[i]==\"''\" or word_cut[i]=='\"':pass\n",
    "            elif i==0 and tagopen!='word':\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'B-'+tagopen\n",
    "            elif tagopen!='word':\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'I-'+tagopen\n",
    "            else:\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'O'\n",
    "            txt5+='\\n'\n",
    "            #j+=1\n",
    "            i+=1\n",
    "        conll2002+=txt5\n",
    "    if pos==False:\n",
    "        return conll2002\n",
    "    return postag(conll2002)\n",
    "# ใช้สำหรับกำกับ pos tag เพื่อใช้กับ NER\n",
    "# print(text2conll2002(t,pos=False))\n",
    "def postag(text):\n",
    "    listtxt=[i for i in text.split('\\n') if i!='']\n",
    "    list_word=[]\n",
    "    for data in listtxt:\n",
    "        list_word.append(data.split('\\t')[0])\n",
    "    #print(text)\n",
    "    list_word=pos_tag(list_word,engine=\"perceptron\", corpus=\"orchid_ud\")\n",
    "    text=\"\"\n",
    "    i=0\n",
    "    for data in listtxt:\n",
    "        text+=data.split('\\t')[0]+'\\t'+list_word[i][1]+'\\t'+data.split('\\t')[1]+'\\n'\n",
    "        i+=1\n",
    "    return text\n",
    "# เขียนไฟล์ข้อมูล conll2002\n",
    "def write_conll2002(file_name,data):\n",
    "    \"\"\"\n",
    "    ใช้สำหรับเขียนไฟล์\n",
    "    \"\"\"\n",
    "    with codecs.open(file_name, \"w\", \"utf-8-sig\") as temp:\n",
    "        temp.write(data)\n",
    "    return True\n",
    "# อ่านข้อมูลจากไฟล์\n",
    "def get_data(fileopen):\n",
    "    \"\"\"\n",
    "    สำหรับใช้อ่านทั้งหมดทั้งในไฟล์ทีละรรทัดออกมาเป็น list\n",
    "    \"\"\"\n",
    "    with codecs.open(fileopen, 'r',encoding='utf-8-sig') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    return [a for a in tqdm(lines) if Unique(a)] # เอาไม่ซ้ำกัน\n",
    "\n",
    "def alldata(lists):\n",
    "    text=\"\"\n",
    "    for data in tqdm(lists):\n",
    "        text+=text2conll2002(data)\n",
    "        text+='\\n'\n",
    "    return text\n",
    "\n",
    "def alldata_list(lists):\n",
    "    data_all=[]\n",
    "    for data in lists:\n",
    "        data_num=[]\n",
    "        try:\n",
    "            txt=text2conll2002(data,pos=True).split('\\n')\n",
    "            for d in txt:\n",
    "                tt=d.split('\\t')\n",
    "                if d!=\"\":\n",
    "                    if len(tt)==3:\n",
    "                        data_num.append((tt[0],tt[1],tt[2]))\n",
    "                    else:\n",
    "                        data_num.append((tt[0],tt[1]))\n",
    "            #print(data_num)\n",
    "            data_all.append(data_num)\n",
    "        except:\n",
    "            print(data)\n",
    "    #print(data_all)\n",
    "    return data_all\n",
    "\n",
    "def alldata_list_str(lists):\n",
    "    string=\"\"\n",
    "    for data in lists:\n",
    "        string1=\"\"\n",
    "        for j in data:\n",
    "            string1+=j[0]+\"\t\"+j[1]+\"\t\"+j[2]+\"\\n\"\n",
    "        string1+=\"\\n\"\n",
    "        string+=string1\n",
    "    return string\n",
    "\n",
    "def get_data_tag(listd):\n",
    "    list_all=[]\n",
    "    c=[]\n",
    "    for i in listd:\n",
    "        if i !='':\n",
    "            c.append((i.split(\"\\t\")[0],i.split(\"\\t\")[1],i.split(\"\\t\")[2]))\n",
    "        else:\n",
    "            list_all.append(c)\n",
    "            c=[]\n",
    "    return list_all\n",
    "def getall(lista):\n",
    "    ll=[]\n",
    "    for i in tqdm(lista):\n",
    "        o=True\n",
    "        for j in ll:\n",
    "            if re.sub(\"\\[(.*?)\\]\",\"\",i)==re.sub(\"\\[(.*?)\\]\",\"\",j):\n",
    "                o=False\n",
    "                break\n",
    "        if o==True:\n",
    "            ll.append(normalize_space(remove_emojis(i)))\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8544/8544 [00:00<00:00, 20489.66it/s]\n",
      "100%|██████████| 8451/8451 [03:34<00:00, 39.38it/s] \n"
     ]
    }
   ],
   "source": [
    "data1=getall(get_data(\"dataset/generated_ner.txt\"))\n",
    "data_not = []\n",
    "datatofile=alldata_list(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open(\"dataset/ner.data\", \"wb\") as dill_file:\n",
    "    dill.dump(datatofile, dill_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate corrupted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E]16.00 น.[/DATE] โด\n",
      "---------------------\n",
      "E]16.00 น.[/DATE] โด\n",
      "---------------------\n",
      "ม่ยืนยัน] [ORGANIZAT\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "for i in data1:\n",
    "    pos = {j.start():['s', i[j.start():j.end()].replace('[', '').replace(']', '').replace('/', '')] \n",
    "           for j in re.finditer('\\[.*?\\]', i) if '/' not in i[j.start():j.end()]}\n",
    "    pos.update({j.start():['e', i[j.start():j.end()].replace('[', '').replace(']', '').replace('/', '')]\n",
    "                for j in re.finditer('\\[.*?\\]', i) if '/' in i[j.start():j.end()]})\n",
    "    pos = sorted(pos.items())\n",
    "    \n",
    "    count = 0\n",
    "    current = ''\n",
    "    for k in pos:\n",
    "        if(k[1][0] == 's'):\n",
    "            count+=1\n",
    "            if(current == ''):\n",
    "                current = k[1][1]\n",
    "            else:\n",
    "                current = 'b'\n",
    "        else:\n",
    "            count-=1\n",
    "            if(current == k[1][1]):\n",
    "                current = ''\n",
    "            else:\n",
    "                current = 'b'\n",
    "        if(current == 'b' or (count > 1 or count < 0)):\n",
    "            print(i[max(k[0]-10, 0):min(k[0]+10, len(i))])\n",
    "            print('---------------------')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:all-in-one]",
   "language": "python",
   "name": "conda-env-all-in-one-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
