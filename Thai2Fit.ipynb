{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import dill\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import chain\n",
    "from sklearn.metrics import classification_report\n",
    "from deeple_preprocessor.tokenize import newmm_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/ner.data', 'rb') as file:\n",
    "    datatofile = dill.load(file)\n",
    "    \n",
    "tagged_sents = []\n",
    "prefixs = ['นาย', 'นาง', 'น.ส.', 'นางสาว', 'น.พ.', 'นพ.', 'พ.ต.ท.']\n",
    "\n",
    "def replace_prefix(text):\n",
    "    for i in prefixs:\n",
    "        text = text.replace(i, '')\n",
    "    return text\n",
    "\n",
    "for i in datatofile:\n",
    "    is_contain_prefix = False\n",
    "    text_inside = []\n",
    "    if(len(i) <= 1):\n",
    "        continue\n",
    "    for idx, j in enumerate(i):\n",
    "        tag = j[2]\n",
    "        if('ไม่ยืนยัน' in j[1]):\n",
    "            tag = 'O'\n",
    "        if(any([prefix in j[0] for prefix in prefixs])):\n",
    "            is_contain_prefix = True\n",
    "        ## Add S and E tag             \n",
    "#         if('B-' in tag):\n",
    "#             if(idx == len(i)-1 or 'I-' not in i[idx+1][1]):\n",
    "#                 tag = tag.replace('B-', 'S-')\n",
    "#         if('I-' in tag):\n",
    "#             if(idx == len(i)-1 or 'I-' not in i[idx+1][1]):\n",
    "#                 tag = tag.replace('I-', 'E-')\n",
    "            \n",
    "        text_inside.append((j[0],tag))\n",
    "    tagged_sents.append(text_inside)\n",
    "    \n",
    "    \n",
    "    if(is_contain_prefix):\n",
    "        dup_text_inside = []\n",
    "        for word, tag in text_inside:\n",
    "            word = replace_prefix(word)\n",
    "            if(word == ''):\n",
    "                continue\n",
    "            else:\n",
    "                dup_text_inside.append((word, tag))\n",
    "        tagged_sents.append(dup_text_inside)\n",
    "    \n",
    "train_sents, test_sents= train_test_split(tagged_sents, test_size=0.1, random_state=112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "thai2fit_model = KeyedVectors.load_word2vec_format('thai2fit/thai2vecNoSym.bin',binary=True)\n",
    "thai2fit_weight = thai2fit_model.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list=[]\n",
    "ner_list=[]\n",
    "thai2dict = {}\n",
    "\n",
    "for sent in train_sents:\n",
    "    for word in sent:\n",
    "        word_list.append(word[0])\n",
    "        ner_list.append(word[1])\n",
    "        \n",
    "for word in thai2fit_model.index2word:\n",
    "    thai2dict[word] = thai2fit_model[word]\n",
    "\n",
    "word_list.append(\"pad\")\n",
    "word_list.append(\"unknown\") #Special Token for Unknown words (\"UNK\")\n",
    "ner_list.append(\"pad\")\n",
    "\n",
    "all_words = sorted(set(word_list))\n",
    "all_ner = sorted(set(ner_list))\n",
    "all_thai2dict = sorted(set(thai2dict))\n",
    "\n",
    "word_to_ix = dict((c, i) for i, c in enumerate(all_words)) #convert word to index \n",
    "ner_to_ix = dict((c, i) for i, c in enumerate(all_ner)) #convert ner to index\n",
    "thai2dict_to_ix = dict((c, i) for i, c in enumerate(thai2dict)) #convert thai2fit to index \n",
    "\n",
    "ix_to_word = dict((v,k) for k,v in word_to_ix.items()) #convert index to word\n",
    "ix_to_ner = dict((v,k) for k,v in ner_to_ix.items())  #convert index to ner\n",
    "ix_to_thai2dict = dict((v,k) for k,v in thai2dict_to_ix.items())  #convert index to thai2fit\n",
    "\n",
    "n_word = len(word_to_ix)\n",
    "n_tag = len(ner_to_ix)\n",
    "n_thai2dict = len(thai2dict_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = set([w_i for w in thai2dict for w_i in w])\n",
    "char2idx = {c: i + 5 for i, c in enumerate(chars)}\n",
    "\n",
    "char2idx[\"pad\"] = 0\n",
    "char2idx[\"unknown\"] = 1\n",
    "char2idx[\" \"] = 2\n",
    "\n",
    "char2idx[\"$\"] = 3\n",
    "char2idx[\"#\"] = 4\n",
    "char2idx[\"!\"] = 5\n",
    "char2idx[\"%\"] = 6\n",
    "char2idx[\"&\"] = 7\n",
    "char2idx[\"*\"] = 8\n",
    "char2idx[\"+\"] = 9\n",
    "char2idx[\",\"] = 10\n",
    "char2idx[\"-\"] = 11\n",
    "char2idx[\".\"] = 12\n",
    "char2idx[\"/\"] = 13\n",
    "char2idx[\":\"] = 14\n",
    "char2idx[\";\"] = 15\n",
    "char2idx[\"?\"] = 16\n",
    "char2idx[\"@\"] = 17\n",
    "char2idx[\"^\"] = 18\n",
    "char2idx[\"_\"] = 19\n",
    "char2idx[\"`\"] = 20\n",
    "char2idx[\"=\"] = 21\n",
    "char2idx[\"|\"] = 22\n",
    "char2idx[\"~\"] = 23\n",
    "char2idx[\"'\"] = 24\n",
    "char2idx['\"'] = 25\n",
    "\n",
    "char2idx[\"(\"] = 26\n",
    "char2idx[\")\"] = 27\n",
    "char2idx[\"{\"] = 28\n",
    "char2idx[\"}\"] = 29\n",
    "char2idx[\"<\"] = 30\n",
    "char2idx[\">\"] = 31\n",
    "char2idx[\"[\"] = 32\n",
    "char2idx[\"]\"] = 33\n",
    "char2idx[\"\\n\"] = 34\n",
    "\n",
    "n_chars = len(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('saved_model/chardict.json', 'w') as chardict:\n",
    "#     json.dump(char2idx, chardict)\n",
    "# with open('saved_model/nerdict.json', 'w') as nerdict:\n",
    "#     json.dump(ner_to_ix, nerdict)\n",
    "# with open('saved_model/thai2dict_to_ix.json', 'w') as f:\n",
    "#     json.dump(thai2dict_to_ix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 284\n",
    "max_len_char = 30\n",
    "\n",
    "character_LSTM_unit = 32\n",
    "char_embedding_dim = 32\n",
    "main_lstm_unit = 256 ## Bidirectional 256 + 256 = 512\n",
    "lstm_recurrent_dropout = 0.5\n",
    "\n",
    "train_batch_size = 32\n",
    "train_epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_word(input_text):\n",
    "    idxs = list()\n",
    "    for word in input_text:\n",
    "        if word in thai2dict:\n",
    "            idxs.append(thai2dict_to_ix[word])\n",
    "        else:\n",
    "            idxs.append(thai2dict_to_ix[\"unknown\"]) #Use UNK tag for unknown word\n",
    "    return idxs\n",
    "\n",
    "def prepare_sequence_target(input_label):\n",
    "    idxs = [ner_to_ix[w] for w in input_label]\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sent =[ [ word[0] for word in sent]for sent in train_sents ] #words only\n",
    "train_targets =[ [ word[1] for word in sent]for sent in train_sents ] #NER only\n",
    "\n",
    "input_test_sent =[ [ word[0] for word in sent]for sent in test_sents ] #words only\n",
    "test_targets =[ [ word[1] for word in sent]for sent in test_sents ] #NER only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Input, TimeDistributed, AdditiveAttention\n",
    "from tensorflow.keras.layers import Dense, SpatialDropout1D, Embedding, concatenate, Dropout\n",
    "from tensorflow_addons.layers import GELU\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from crf.crf import CRF\n",
    "from crf.crf_losses import crf_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9008/9008 [00:33<00:00, 268.17it/s]\n"
     ]
    }
   ],
   "source": [
    "## Word Training\n",
    "X_word_tr = [prepare_sequence_word(s) for s in input_sent]\n",
    "X_word_tr = pad_sequences(\n",
    "    maxlen=max_len, sequences=X_word_tr, value=thai2dict_to_ix[\"pad\"], padding='post', truncating='post'\n",
    ")\n",
    "\n",
    "## Character Training\n",
    "X_char_tr = []\n",
    "for sentence in tqdm(train_sents):\n",
    "    sent_seq = []\n",
    "    for i in range(max_len):\n",
    "        word_seq = []\n",
    "        for j in range(max_len_char):\n",
    "            try:\n",
    "                if(sentence[i][0][j] in char2idx):\n",
    "                    word_seq.append(char2idx.get(sentence[i][0][j]))\n",
    "                else:\n",
    "                    word_seq.append(char2idx.get(\"unknown\"))\n",
    "            except:\n",
    "                word_seq.append(char2idx.get(\"pad\"))\n",
    "        sent_seq.append(word_seq)\n",
    "    X_char_tr.append(np.array(sent_seq))\n",
    "\n",
    "## Sequence Label Training\n",
    "y_tr = [prepare_sequence_target(s) for s in train_targets]\n",
    "y_tr = pad_sequences(maxlen=max_len, sequences=y_tr, value=ner_to_ix[\"pad\"], padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1001/1001 [00:03<00:00, 252.59it/s]\n"
     ]
    }
   ],
   "source": [
    "## Word Testing\n",
    "X_word_te = [prepare_sequence_word(s) for s in input_test_sent]\n",
    "X_word_te = pad_sequences(maxlen=max_len, sequences=X_word_te, value=thai2dict_to_ix[\"pad\"], padding='post', truncating='post')\n",
    "\n",
    "## Character Testing\n",
    "X_char_te = []\n",
    "for sentence in tqdm(test_sents):\n",
    "    sent_seq = []\n",
    "    for i in range(max_len):\n",
    "        word_seq = []\n",
    "        for j in range(max_len_char):\n",
    "            try:\n",
    "                if(sentence[i][0][j] in char2idx):\n",
    "                    word_seq.append(char2idx.get(sentence[i][0][j]))\n",
    "                else:\n",
    "                    word_seq.append(char2idx.get(\"unknown\"))\n",
    "            except:\n",
    "                word_seq.append(char2idx.get(\"pad\"))    \n",
    "        sent_seq.append(word_seq)\n",
    "    X_char_te.append(np.array(sent_seq))\n",
    "\n",
    "## Sequence Label Testing\n",
    "y_te = [prepare_sequence_target(s) for s in test_targets]\n",
    "y_te = pad_sequences(maxlen=max_len, sequences=y_te, value=ner_to_ix[\"pad\"], padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         [(None, 284, 30)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_input_ (InputLayer)        [(None, 284)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 284, 30, 32)  12800       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, 284, 400)     22270800    word_input_[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 284, 64)      16640       time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 284, 464)     0           word_embedding[0][0]             \n",
      "                                                                 time_distributed_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 284, 464)     0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 284, 512)     1476608     spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, 284, 50)      25650       bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_2 (CRF)                     (None, 284)          2378        time_distributed_8[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 23,804,876\n",
      "Trainable params: 1,534,076\n",
      "Non-trainable params: 22,270,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Word Input\n",
    "word_in = Input(shape=(max_len,), name='word_input_')\n",
    "\n",
    "# Word Embedding Using Thai2Fit\n",
    "word_embeddings = Embedding(input_dim=n_thai2dict,\n",
    "                            output_dim=400,\n",
    "                            weights = [thai2fit_weight],input_length=max_len,\n",
    "                            mask_zero=False,\n",
    "                            name='word_embedding', trainable=False)(word_in)\n",
    "\n",
    "# Character Input\n",
    "char_in = Input(shape=(max_len, max_len_char,), name='char_input')\n",
    "\n",
    "# Character Embedding\n",
    "emb_char = TimeDistributed(Embedding(input_dim=n_chars, output_dim=char_embedding_dim, \n",
    "                           input_length=max_len_char, mask_zero=False))(char_in)\n",
    "\n",
    "# Character Sequence to Vector via BiLSTM\n",
    "char_enc = TimeDistributed(Bidirectional(LSTM(units=character_LSTM_unit, return_sequences=False, recurrent_dropout=lstm_recurrent_dropout)))(emb_char)\n",
    "\n",
    "\n",
    "# Concatenate All Embedding\n",
    "all_word_embeddings = concatenate([word_embeddings, char_enc])\n",
    "all_word_embeddings = SpatialDropout1D(0.3)(all_word_embeddings)\n",
    "\n",
    "# Main Model Dense attention\n",
    "main_lstm = Bidirectional(LSTM(units=main_lstm_unit, return_sequences=True, recurrent_dropout=lstm_recurrent_dropout))(all_word_embeddings)\n",
    "main_lstm = TimeDistributed(Dense(50, activation=\"relu\"))(main_lstm)\n",
    "\n",
    "# CRF\n",
    "out = CRF(n_tag)(main_lstm)  # CRF layer\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=[word_in, char_in], outputs=out)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=crf_loss, metrics=[Accuracy()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stopper = EarlyStopping(patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9008 samples, validate on 1001 samples\n",
      "Epoch 1/2\n",
      "8992/9008 [============================>.] - ETA: 1s - loss: 45.4122 - accuracy: 0.9615\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.97854, saving model to saved_model/weights-improvement-01-0.961.hdf5\n",
      "9008/9008 [==============================] - 994s 110ms/sample - loss: 45.3661 - accuracy: 0.9615 - val_loss: 19.3822 - val_accuracy: 0.9785\n",
      "Epoch 2/2\n",
      "8992/9008 [============================>.] - ETA: 1s - loss: 16.0534 - accuracy: 0.9818\n",
      "Epoch 00002: val_accuracy improved from 0.97854 to 0.98700, saving model to saved_model/weights-improvement-02-0.982.hdf5\n",
      "9008/9008 [==============================] - 789s 88ms/sample - loss: 16.0448 - accuracy: 0.9818 - val_loss: 11.2201 - val_accuracy: 0.9870\n"
     ]
    }
   ],
   "source": [
    "filepath=\"saved_model/weights-improvement-{epoch:02d}-{accuracy:.3f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit(\n",
    "    [X_word_tr, np.array(X_char_tr).reshape((len(X_char_tr), max_len, max_len_char))],\n",
    "    y_tr,\n",
    "    batch_size=train_batch_size, epochs=1, verbose=1,callbacks=callbacks_list,\n",
    "    validation_data=(\n",
    "        [X_word_te, np.array(X_char_te).reshape((len(X_char_te), max_len, max_len_char))],\n",
    "        y_te\n",
    "    ),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_filepath=\"saved_model/last_weight-50.hdf5\"\n",
    "# model.save_weights(save_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_filepath=\"saved_model/weights-improvement-53-0.993.hdf5\"\n",
    "model.load_weights(load_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001/1001 [==============================] - 15s 15ms/sample\n"
     ]
    }
   ],
   "source": [
    "pred_model = model.predict([X_word_te,np.array(X_char_te).reshape((len(X_char_te),max_len, max_len_char))], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for i in range(0,len(pred_model)):\n",
    "    try:\n",
    "        out = pred_model[i]\n",
    "        true = y_te[i]\n",
    "        revert_pred=[ix_to_ner[i] for i in out]\n",
    "        revert_true=[ix_to_ner[i] for i in true]\n",
    "        y_pred.append(revert_pred)\n",
    "        y_true.append(revert_true)\n",
    "    except:\n",
    "        print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_classification_report(y_true, y_pred):\n",
    " \n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "    tagset = list(sorted(set(lb.classes_)))\n",
    "    tagset = [i for i in tagset if len(i.split('-')) == 2]\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    print(list(sorted(set(lb.classes_))))\n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "        digits=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-DATE', 'B-EMAIL', 'B-LAW', 'B-LEN', 'B-LOCATION', 'B-MONEY', 'B-ORGANIZATION', 'B-PERCENT', 'B-PERSON', 'B-PHONE', 'B-TIME', 'B-URL', 'B-ZIP', 'I-DATE', 'I-EMAIL', 'I-LAW', 'I-LEN', 'I-LOCATION', 'I-MONEY', 'I-ORGANIZATION', 'I-PERCENT', 'I-PERSON', 'I-PHONE', 'I-TIME', 'I-URL', 'O', 'pad']\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        B-DATE     0.7710    0.8312    0.8000       397\n",
      "       B-EMAIL     0.0000    0.0000    0.0000         1\n",
      "         B-LAW     0.0000    0.0000    0.0000        36\n",
      "         B-LEN     0.0000    0.0000    0.0000        12\n",
      "    B-LOCATION     0.8060    0.6639    0.7281       851\n",
      "       B-MONEY     0.8571    0.8696    0.8633        69\n",
      "B-ORGANIZATION     0.7507    0.7062    0.7278      1096\n",
      "     B-PERCENT     0.0000    0.0000    0.0000        20\n",
      "      B-PERSON     0.7485    0.7415    0.7450       851\n",
      "       B-PHONE     0.0000    0.0000    0.0000        21\n",
      "        B-TIME     0.7090    0.5249    0.6032       181\n",
      "         B-URL     0.0000    0.0000    0.0000        13\n",
      "         B-ZIP     0.0000    0.0000    0.0000         1\n",
      "        I-DATE     0.7568    0.9314    0.8351       802\n",
      "       I-EMAIL     0.0000    0.0000    0.0000         2\n",
      "         I-LAW     0.0000    0.0000    0.0000       139\n",
      "         I-LEN     0.0000    0.0000    0.0000        24\n",
      "    I-LOCATION     0.8420    0.5552    0.6692       960\n",
      "       I-MONEY     0.8594    0.9270    0.8919       178\n",
      "I-ORGANIZATION     0.8426    0.5819    0.6884      1325\n",
      "     I-PERCENT     0.0000    0.0000    0.0000        25\n",
      "      I-PERSON     0.8271    0.9231    0.8725      3109\n",
      "       I-PHONE     0.0000    0.0000    0.0000        66\n",
      "        I-TIME     0.8487    0.5706    0.6824       354\n",
      "         I-URL     0.7265    0.9942    0.8395       171\n",
      "\n",
      "     micro avg     0.8012    0.7393    0.7690     10704\n",
      "     macro avg     0.4138    0.3928    0.3979     10704\n",
      "  weighted avg     0.7773    0.7393    0.7491     10704\n",
      "   samples avg     0.0278    0.0278    0.0278     10704\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thirasan/opt/miniconda3/envs/addres-ner/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/thirasan/opt/miniconda3/envs/addres-ner/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/thirasan/opt/miniconda3/envs/addres-ner/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.array(y_pred)\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "print(ner_classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:addres-ner] *",
   "language": "python",
   "name": "conda-env-addres-ner-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
